---
title: "Modelling Pavlovian Attentional Biases Using Reinforcement Learning"
author:
  - name: "Francisco Garre-Frutos"
    acknowledgements: "Mind, Brain and Behavior Research Center (CIMCYC) and Department of Experimental Psychology, University of Granada, Granada, Spain" # workaround
    affiliations:
      - name: "University of Granada"
        address: "Granada, Spain"
    email: "fgfrutos@ugr.es"
  - name: "David Luque"
    acknowledgements: "Department of Basic Psychology and Málaga Institute of Biomedical Research (IBIMA), University of Málaga, Málaga, Spain" # workaround
    affiliations:
      - name: "University of Malaga"
        address: "Málaga, Spain"
    email: "dluque@uma.es"
    note: "T"
  - name: "Pablo Martínez-López"
    url: "A" #workaround
    affiliations:
      - name: "University of Malaga"
        address: "Málaga, Spain"
    email: "mlpablocorreo@gmail.com"
  - name: "Juan Lupiáñez"
    orcid: "A" # workaround
    affiliations:
      - name: "University of Granada"
        address: "Granada, Spain"
    email: "jlupiane@ugr.es"
  - name: "Miguel A. Vadillo"
    acknowledgements: "Department of Basic Psychology, Autonomous University of Madrid, Madrid, Spain" # workaround
    affiliations:
      - name: "Autonomous University of Madrid"
        address: "Madrid, Spain"
    email: "miguel.vadillo@uam.es"
    attributes:
      corresponding: true
abstract: |
 A central question in modern models of Pavlovian learning is how learning influences attention. Mackintosh’s theory proposes that stimuli that reliably predict significant outcomes receive more attention to maximize learning, whereas Pearce-Hall’s theory proposes that uncertainty drives attention to promote exploration. Empirical evidence supports both views, showing that stimuli that consistently predict reward or reward variability are more likely to capture attention, even when they act as distractors in conflict with other goal-relevant stimuli. Although these findings are often assumed to reflect the mechanisms of Mackintosh and Pearce-Hall, there have been surprisingly few attempts to explicitly model the learning dynamics underlying such attentional biases. In this work, we developed two hierarchical Bayesian models implementing Mackintosh and Pearce-Hall principles and compared their fit to two datasets in which either value or uncertainty was manipulated. Surprisingly, our results show that the Pearce-Hall model can account for both experimental effects. We argue that this likely reflects a methodological confound in how value is typically manipulated, as high-value cues often also exhibit greater outcome variance, raising the possibility that some experimental effects reflect uncertainty-driven rather than value-driven attention.
keywords: ["reinforcement learning,", "Pavlovian,", "value,", "uncertainty,", "attention"]
acknowledgements: |
  This work was supported by an FPU predoctoral grant (ref. FPU20/00826) to FGF.
format:
  Rldm25_vmac-pdf:
    keep-tex: true
bibliography: references.bib
---
```{r}
if (!require(pacman)) {
  install.packages("pacman")
  library(pacman)
}

p_load(
  kableExtra,
  loo,
  dplyr,
  tidyr,
  ggplot2,
  showtext,
  sysfonts,
  ggh4x
)
```

# Introduction

The relationship between learning and attention has been a central question in Pavlovian learning for decades. Since @rescorlaw72, numerous accounts have attempted to incorporate attention into associative learning models. For example, @mackintosh1975 proposed that attention increases for cues that reliably predict significant outcomes, thereby promoting further learning about those cues, whereas @pearce1980 argued that attention increases for cues associated with greater uncertainty or variability in outcomes, thereby promoting exploratory learning. Both principle have received empirical support. For instance, in the study by @lepelley2015, participants performed a visual search task in which singleton distractors were consistently paired with two different reward magnitudes. In this procedure, although the distractors predicted reward, looking directly at them caused omission of reward, even though attending to the high-reward predictive distractor was counterproductive to the task goals, @lepelley2015 found that participants tended to look more at high than the low-value distractor, reflecting a Pavlovian bias that contradicted the participants' goals.

According to @lepelley2016, this attentional bias could be explained by a modified Mackintosh model, in which the associative strength ($V$) of a distractor is described by the following Rescorla-Wagner rule:
$$ 
V_{n} = V_{n-1} + \eta\,\alpha_{n}\,\bigl(\lambda_{n} - V_{n-1}\bigr)
$$ {#eq-1}
where $\eta$ is a learning rate parameter, $\lambda$ is the reward in trial $n$, and $\alpha$ is a weight parameter reflecting attention to the cue. From this formula, $V_{n}$ is updated as a function of the prediction error, $\bigl(\lambda_{n} - V_{n-1}\bigr)$. Unlike other formulations of the Mackintosh model, @lepelley2016 assumed that $\alpha$ is updated as
$$
\alpha_{n} = \lvert V_{n-1}\rvert
$$ {#eq-2}
In other words, attention to the cue is a function of the asymptotic $\lambda$. This formulation of the Mackintosh model can explain why value-driven distraction increases over trials and remains stable with training.

In contrast, @pearce1982 postulated that attention is mainly driven by prediction errors:
$$
\alpha_{n} = \gamma\,\lvert \lambda_{n} - V_{n-1}\rvert 
+ \bigl(1 - \gamma\bigr)\,\alpha_{n-1}
$$ {#eq-3}
where $\gamma$ is a decay parameter that weights the relevance of previous values of $\alpha$. The Pearce-Hall principle predicts that cues associated with more variable outcomes (and thus higher prediction errors) will receive more attention. Consistent with this prediction, and using a procedure similar to Le Pelley et al. (2015), @pearson2024 showed that when irrelevant distractors were associated with different levels of reward variability, high-variance distractors received more attention than low-variance distractors, even when the overall expected value was higher for low-variance distractors [see also @lepelley2019].

The results of both Le Pelley et al. (2015) and @pearson2024 suggest that the principles of Mackintosh and Pearce-Hall may be applicable in different contexts. Despite explicit formulations of how attention should change as a function of learning, there are few attempts to explicitly model the learning dynamics associated with either theory in Pavlovian attentional biases. In light of this, in this work, we developed two hierarchical Bayesian reinforcement learning (RL) models in which attention is modeled according to formulas (2) and (3). We then fit these two models to two openly aviable datasets where either reward value or uncertainty is manipulated, and compared the peformance of each model to describe the data.

# Method

## Data

We used the open-source data from Experiment 2 of the @le2024 study and Experiment 1 of the @chow2024 study. In both experiments, participants performed the additional singleton task [@theeuwes1992]. In each trial, participants had to search for unique diamond-shaped stimulus surrounded by circle non-target shapes, one of which was a color-singleton distractor (a circle of a different color) (Figure 1). Critically, during the task, participants could earn rewards (points) depending on the color of the distractor. In the @le2024 study, if participants managed to look at the target stimuli without looking at the distractor, they would earn either a high or low reward depending on the color of the distractor (a high-reward or a low-reward color). However, looking at the distractor would result in the omission of the reward. In @chow2024, the color of the distractor signaled outcome variability. One distractor color was always associated with the same outcome (low variance), while the other singleton distractor signaled two possible outcomes (high variance) with a 50% probability. The expected value of both distractors, however, was matched overall.

```{r}
#| label: fig-1
#| fig-cap: Schematic representation of the task design. In Le et al. (2024), the high-value distractor (top) was associated with more reward than the low-value distractor (bottom), although looking at the distractor before the target produced reward omission. In Chow et al. (2024), the high-variance distractor was associated with two possible outcomes at 50% probability, while the low-variance distractor was always associated with the same outcome.
#| fig-width: 12
#| fig-height: 10
knitr::include_graphics("Output/figures/figure_1.png")
```

We employed this two specific datasets because they are directly comparable in design and have an unusually large number of participants compared to much of the literature. In Experiment 2 of @le2024, there are 84 participants with 384 trials each, whereas in @chow2024, there are 40 participants with 512 trials each. We provide the data and analysis code for this study in the following repository: [https://github.com/franfrutos/rldm25_vmac](https://github.com/franfrutos/rldm25_vmac).

## Model Specification

For the datasets described above, we model whether a participant looked at the distractor (1) or the target (0) on trial $i$. This is implemented using a logistic regression ($\text{logit}^{-1}$ function), where $\operatorname{Pr}(\text{Look}_{i} = 1)$ is determined by the latent attention ($\alpha$) for each distractor ($s$):
$$
\begin{aligned}
\operatorname{Pr}(\text{Look}_{i} = 1) \sim \text{Bernoulli}(\theta_{i}),\\
\theta_{i} = \frac{1}{1 + e^{-(\beta_{0,j} + \beta_{1,j} \cdot \alpha_{s[i]} + \beta_{2,j} \cdot t_{i})}},
\end{aligned}
$$ {#eq-4}
where $\beta_{0,j}$ is the intercept for subject $j$, $\beta_{1,j}$ is the slope that relates $\alpha$ to the probability of looking at the distractor, and $\beta_{2,j}$ controls for practice effects across trials ($t$). This simple logistic function can be viewed as a softmax rule with two levels: the target (reference) and the distractor. Thus, $\beta_{0}$ can be interpreted as a bias term (a tendency to choose the target or the distractor), and $\beta_{1}$ as an *inverse temperature* parameter. We parameterized $\beta_{1}$ as $\text{logit}^{-1}(\cdot)\cdot 10$, so it could only take positive values in the range \[0, 10\]. A higher $\beta_{1}$ increases selection of distractors with higher $\alpha$.

On trial $i$, each distractor $s$ updates its $V_{s}$ according to equation (1), where $\lambda_{i}$ equals the observed reward[^1] on that trial. Regarding $\alpha_{s}$, both models assume that participants start with an initial level of attention ($\alpha_0$). Then, in the *Mackintosh* model (Le Pelley et al., 2016), $\alpha$ is updated according to equation (2), while in the *Pearce-Hall* model, $\alpha$ is updated following equation (3), where $\eta$ and $\gamma$ are subject-specific parameters governing the update of $V$ and $\alpha$. 

[^1]: $\lambda$ is scaled by $\frac{\lambda_{i}}{\max(\lambda)}$, which ensures that $V$ takes values in the range \[0, 1\].

All subject-specific parameters ($k$) are assumed to be drawn from a normal distribution, using non-centered parameterization to improve sampling efficiency:
$$
\beta_{k,j} = \mu_{k} + \sigma_{k} \cdot z_{k,j}, \quad z_{k,j} \sim \mathcal{N}(0, 1).
$$ {#eq-5}
Subject-specific parameters ($\beta_{k,j}$) are modeled by scaling group-level means ($\mu_{k}$) with individual deviations ($\sigma_k$) and standardized parameters ($z_{k,j}$). Bounded parameters ($\eta$, $\gamma$, $\alpha_{0}$, and $\beta_{1}$) are then transformed using the $\text{logit}^{-1}$ function to ensure they remain within the \[0, 1\] interval. We used weakly informative priors ($\mathcal{N}(0, 1)$) for group-level means $\mu$, and truncated normals for $\sigma$.

# Results

The models described above were programmed in Stan [@stan2024]. We fit both models to each dataset using 6000 warm-up iterations and 8000 samples across four chains ($\bar{R} < 1.01$). We assessed the relative fit of the two models to each dataset using the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO; Vehtari et al., 2017). Table 1 shows the relative performance of the *Mackintosh* model compared to the *Pearce-Hall* model, where a negative ELPD means worse performance.

```{r}
#| table: relative-fit
loo_estimates <- readRDS("Output/results/loo_estimates.rds")

table_data <- cbind(
  as_tibble(rbind(loo_compare(loo_estimates[[2]], loo_estimates[[4]]))[, 1:2]) %>%
    mutate(Data = "Chow et al. (2024)",
           Model = c("Pearce-Hall", "Mackintosh")) %>%
    pivot_wider(names_from = Data, values_from = c(elpd_diff, se_diff)) %>%
    dplyr::rename(elpd_chow = "elpd_diff_Chow et al. (2024)",
           se_chow = "se_diff_Chow et al. (2024)"),
  as_tibble(rbind(loo_compare(loo_estimates[[1]], loo_estimates[[3]]))[, 1:2]) %>%
    mutate(Data = "Le et al. (2024)",
           Model_2 = c("Pearce-Hall", "Mackintosh")) %>%
    pivot_wider(names_from = Data, values_from = c(elpd_diff, se_diff)) %>%
    dplyr::rename(elpd_le = "elpd_diff_Le et al. (2024)",
           se_le = "se_diff_Le et al. (2024)")
) %>%
  mutate_if(is.numeric, round, digits = 0)

table_data$Model_2 <- NULL

formatted_colnames <- c("Model", 
                        "$\\Delta$ELPD", "$\\Delta$SE", 
                        "$\\Delta$ELPD", "$\\Delta$SE")

kable_table <- table_data %>%
  kableExtra::kable("latex", 
                    col.names = formatted_colnames, 
                    booktabs = TRUE, 
                    escape = FALSE, 
                    caption = "Relative fit analysis",
                    align = c("l", "c", "c", "c", "c")) %>%
  kableExtra::add_header_above(
    c(" " = 1, "Chow et al. (2024)" = 2, "Le et al. (2024)" = 2), bold = FALSE
  ) %>%
  kableExtra::kable_styling(latex_options = c("hold_position"), 
                            font_size = 10, 
                            position = "center", 
                            full_width = TRUE) %>%
  footnote(
    general_title = "Note.",
    general = "ELPD = Expected Log-Pointwise Predictive Density. SE = Standard Error.",
    footnote_as_chunk = TRUE
  ) %>%
  kableExtra::row_spec(0, bold = FALSE, extra_latex_after = "\\addlinespace")

kable_table
```

According to @vehtari2017, if the difference in ELPD between models deviates by more than 2 SEs, the model with the higher ELPD is likely to be the better fit for the data. By this criterion, the *Pearce-Hall* model outperformed the *Mackintosh* model in both datasets.

```{r}
#| label: fig-2
#| fig-cap: Posterior Predictive Checks (PPC) for the *Mackintosh* and *Pearce-Hall* models, split by dataset. PPCs were generated by simulating new observations based on the fitted parameters and aggregating results at the group level. Solid lines represent the mean, and the shaded area is the 89% High Density Interval (HDI) of the posterior predictions across epochs of 32 trials. The dots represent the observed mean proportion of gazes at the distractor, while the error bars show the SEM. 
#| fig-width: 10
#| fig-height: 7

dPlots <- readRDS("Output/figures/dPlots.rds")
ggplot(data = as_tibble(dPlots[[1]]), aes(y = P, x = Epoch, fill = Singleton, color = Singleton)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = Low, ymax = High), alpha = 0.2, color = NA) + 
  geom_pointrange(data = as_tibble(dPlots[[2]]), aes(
    ymin = P - se, ymax = P + se, color = Singleton,
    x = as.numeric(Epoch)
  ), shape = 1, size = 0.5) + 
  coord_cartesian(ylim = c(0, 0.5)) +
  scale_x_continuous(breaks = 1:16) + 
  scale_color_manual(values = c("#C15F1E", "#258DA5")) +
  scale_fill_manual(values = c("#C15F1E", "#258DA5")) +
  facet_grid2(Model ~ Data, scales = "free_x", independent = "x") + 
  theme_bw(base_size = 14, base_family = "Palatino") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "#F2F2F2"),
    strip.text = element_text(size = 14),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 14),
    legend.position = c(0.85, 0.95),
    legend.direction = "horizontal"
  ) +
  facetted_pos_scales(
    x = list(scale_x_continuous(breaks = 0), scale_x_continuous(breaks = 0))
  ) +
  labs(
    x = "Epoch", 
    y = "Pr(Look = 1)",
    fill = "Distractor", 
    color = "Distractor"
  )
```

Although the previous analysis suggests that *Pearce-Hall* has the best relative fit for @le2024 and @chow2024 datasets, we should also verify whether both models predict the theoretically relevant experimental effects—namely, whether participants look more often at the high-value (or high-variance) distractor relative to the low-value (or low-variance) distractor. To examine this, we compared the absolute fit of the *Mackintosh* and *Pearce-Hall* models via Posterior Predictive Checks (PPC). Figure 2 displays the PPCs as a function of distractor type and epochs (32-trial bins). As expected, the *Pearce-Hall* model alone captures the greater attention to the high-variance distractor in the @chow2024 dataset, whereas the *Mackintosh* model predicts no differences between distractors, because the asymptotic $\lambda$ was matched. Interestingly, although the *Mackintosh* model provides a reasonable fit to @le2024 data, the *Pearce-Hall* model's predictions are closer. It also appears to better capture the earlier stages of learning, which may explain why it provides superior predictive accuracy.

# Conclusions

Our results show that it is possible to use RL models to capture Pavlovian attentional biases. We found that when uncertainty is manipulated [@chow2024], only the *Pearce-Hall* model explains the observed data. When value is manipulated [@le2024], somewhat unexpectedly, *Pearce-Hall* also predicts greater attention to the high-value distractor. As suggested by Pearson et al. (2024), outcome variance is a measure of uncertainty because it reflects the amount of prediction error associated with a cue. While outcome variance is explicitly manipulated in @pearson2024, in the paradigm used by @lepelley2015, manipulation of value can be confounded with differences in outcome variance between distractors. As explained above, in @lepelley2015, when participants look at the distractors, they cause a reward omission, which can be viewed as a prediction error. This prediction error is larger for the high-value distractor than for the low-value distractor, and the proportion of prediction errors increases as a function of attention because the probability of looking at a distractor increases across trials only for the high-value distractor. Thus, the high-value distractor not only represents the highest value, but is also associated with greater variability. This methodological confound may explain why the *Pearce-Hall* principle is a better fit for @le2024.

# References

::: {#refs}
:::